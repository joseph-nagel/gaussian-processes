{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01c32a4-aef0-4637-9764-288cd668359a",
   "metadata": {},
   "source": [
    "# GP regression\n",
    "\n",
    "An interesting application of GPs is as nonparametric models for supervised learning. When considering regression problems, this is usually referred to as **GP regression** (GPR) or less commonly as **Kriging**. A small intro to this subject is given below. Moreover, a very simple practical problem based on a sine function is addressed thereafter.\n",
    "\n",
    "One sometimes distinguishes between a **noise-free** and a **noisy** observations. The former scenario assumes that the data set $\\{(\\boldsymbol{x}_i, y_i)\\}_{i=1}^N$ contains noise-free measurements $y_i = f(\\boldsymbol{x}_i)$ of function values at various locations $\\boldsymbol{x}_i$. The joint distribution of the GP at those data locations and at other locations can be written as\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{f} \\\\\n",
    "\\boldsymbol{g}\n",
    "\\end{pmatrix} \\sim\n",
    "\\mathcal{N} \\left(\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{\\mu}_\\boldsymbol{f} \\\\\n",
    "\\boldsymbol{\\mu}_\\boldsymbol{g}\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}} &\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{g}} \\\\\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{f}} &\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{g}}\n",
    "\\end{pmatrix}\n",
    "\\right).\n",
    "$$\n",
    "Here, $\\boldsymbol{f} = (f(\\boldsymbol{x}_1), \\ldots, f(\\boldsymbol{x}_N))^\\top$ are the random variables that are measured, whereas $\\boldsymbol{g} = (g(\\boldsymbol{x}^\\star_1), \\ldots, g(\\boldsymbol{x}^\\star_M))^\\top$ denotes the unobserved random variables at some test locations $\\{\\boldsymbol{x}^\\star_i\\}_{i=1}^M$. The corresponding marginals of this **prior model** are $\\boldsymbol{f} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{f}, \\boldsymbol{\\Sigma}_\\boldsymbol{f})$ and $\\boldsymbol{g} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{g}, \\boldsymbol{\\Sigma}_\\boldsymbol{g})$. Given realizations of the observed variables, one can obtain the conditional distribution of the unobserved variables. Such **posterior predictions** are given as\n",
    "$$\n",
    "\\boldsymbol{g} | \\boldsymbol{f} \\sim\n",
    "\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{g} | \\boldsymbol{f}},\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} | \\boldsymbol{f}}), \\quad\n",
    "\\boldsymbol{\\mu}_{\\boldsymbol{g} | \\boldsymbol{f}} =\n",
    "\\boldsymbol{\\mu}_\\boldsymbol{g} + \\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{f}}\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}}^{-1} (\\boldsymbol{f} - \\boldsymbol{\\mu}_\\boldsymbol{f}), \\quad\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} | \\boldsymbol{f}} =\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{g}} - \\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{f}}\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}}^{-1} \\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{g}}.\n",
    "$$\n",
    "\n",
    "The standard model for noisy data assumes that the measurements $y(\\boldsymbol{x}_i) = f(\\boldsymbol{x}_i) + \\epsilon_i$ are subject to Gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$. Based on some standard independence assumptions, such that one has $\\boldsymbol{y} | \\boldsymbol{f} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{f}, \\boldsymbol{\\Sigma}_\\boldsymbol{f})$ and $\\boldsymbol{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{f}, \\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}} + \\sigma_\\epsilon^2 \\boldsymbol{I})$, the prior model for the observed data $\\boldsymbol{y}$ and the unobserved function values $\\boldsymbol{g}$ simply is\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{y} \\\\\n",
    "\\boldsymbol{g}\n",
    "\\end{pmatrix} \\sim\n",
    "\\mathcal{N} \\left(\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{\\mu}_\\boldsymbol{f} \\\\\n",
    "\\boldsymbol{\\mu}_\\boldsymbol{g}\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}} + \\sigma_\\epsilon^2 \\boldsymbol{I} &\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{g}} \\\\\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{f}} &\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{g}}\n",
    "\\end{pmatrix}\n",
    "\\right).\n",
    "$$\n",
    "It is remarked that this treatment of noise is very similar to adding a white noise component to the GP kernel. This would impact the diagonal entries of both $\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}}$ and $\\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{g}}$. Such a **nugget term** is sometimes introduced for numerical stability. In any case, conditioning on the measurements yields the posterior predictions once again\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{g} | \\boldsymbol{y} &\\sim\n",
    "\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{g} | \\boldsymbol{y}},\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} | \\boldsymbol{y}}), \\\\\n",
    "\\boldsymbol{\\mu}_{\\boldsymbol{g} | \\boldsymbol{y}} &=\n",
    "\\boldsymbol{\\mu}_\\boldsymbol{g} + \\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{f}}\n",
    "\\left( \\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}} + \\sigma_\\epsilon^2 \\boldsymbol{I} \\right)^{-1}\n",
    "\\left( \\boldsymbol{f} - \\boldsymbol{\\mu}_\\boldsymbol{f} \\right), \\\\\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} | \\boldsymbol{y}} &=\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{g}} - \\boldsymbol{\\Sigma}_{\\boldsymbol{g} \\boldsymbol{f}}\n",
    "\\left( \\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}} + \\sigma_\\epsilon^2 \\boldsymbol{I} \\right)^{-1}\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{g}}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "It is often difficult to specify all hyperparameters of a GP prior on an ad hoc basis. One may therefore resort to **model selection** or **hyperparameter optimization** in order to find \"good\" values. In the present purely Gaussian context, one can for instance maximize (the logarithm of) the **marginal likelihood** as a function of the hyperparameters. Let us collect the parameters of the covariance $\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}}(\\ell, \\sigma)$ and the noise level $\\sigma_\\epsilon$ into $\\boldsymbol{\\theta} = (\\ell, \\sigma, \\sigma_\\epsilon)$. The likelihood as function of $\\boldsymbol{\\theta}$ is then explicitly given as\n",
    "$$\n",
    "p_{\\boldsymbol{\\theta}}(\\boldsymbol{y}) = \\int p_{\\sigma_\\epsilon}(\\boldsymbol{y} | \\boldsymbol{f}) \\,\n",
    "p_{\\ell, \\sigma}(\\boldsymbol{f}) \\, \\mathrm{d} \\boldsymbol{f} =\n",
    "\\mathcal{N}(\\boldsymbol{y} | \\boldsymbol{\\mu}_\\boldsymbol{f},\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{f} \\boldsymbol{f}}(\\ell, \\sigma) + \\sigma_\\epsilon^2 \\boldsymbol{I}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10a5c8-2e02-4908-b874-5abb271a47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1e264-963b-4679-b295-8eb22266b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "from utils.modules import ExactInferenceGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2987d0-c5ca-4489-a575-cbf5d8f7bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91b99c-03ff-418b-8615-95553fe4251f",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfa440-6f83-494e-83eb-b513c192cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    '''Calculate ground truth.'''\n",
    "    y = torch.sin(2 * torch.pi * x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6c438-9653-4754-837b-d3b9bb00414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "noise_std = 0.1\n",
    "\n",
    "x_train = torch.rand(num_samples)\n",
    "y_train = f(x_train) + noise_std * torch.randn_like(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e132c-86c4-4a17-876e-7b835df0dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = torch.linspace(-0.25, 1.25, 1001)\n",
    "y_values = f(x_values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(x_values.numpy(), y_values.numpy(), alpha=0.8, label='true function')\n",
    "ax.scatter(x_train.numpy(), y_train.numpy(), alpha=0.8, label='training data')\n",
    "ax.set(xlabel='x', ylabel='y')\n",
    "ax.set_xlim((x_values.min(), x_values.max()))\n",
    "ax.legend()\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f0e2b-058e-4dac-9121-0bee498178c2",
   "metadata": {},
   "source": [
    "## Prior model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f51524-8653-44e6-b55e-410ad5ea8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model = ExactInferenceGP(\n",
    "    x_train=None,\n",
    "    y_train=None,\n",
    "    prior_var=0.4,\n",
    "    prior_length=0.1,\n",
    "    noise_var=0.001\n",
    ")\n",
    "\n",
    "print('Prior length: {:.4f}'.format(prior_model.prior_length.item()))\n",
    "print('Prior std.: {:.4f}'.format(torch.sqrt(prior_model.prior_var).item()))\n",
    "print('Noise std.: {:.4f}'.format(torch.sqrt(prior_model.noise_var).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a15a8-4bbb-4b4a-9ea7-27b4c05f307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model.eval() # eval mode for posterior predictions\n",
    "# prior_model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    post_mvn = prior_model(x_values) # compute posterior predictions in eval mode\n",
    "    pred_mvn = prior_model.likelihood(post_mvn) # include likelihood data model\n",
    "\n",
    "    pred_mean = pred_mvn.mean\n",
    "    pred_std = pred_mvn.stddev\n",
    "    pred_var = pred_mvn.variance\n",
    "    pred_cov = pred_mvn.covariance_matrix\n",
    "    pred_lower, pred_upper = pred_mvn.confidence_region() # two stds.\n",
    "\n",
    "    pred_samples = post_mvn.sample(sample_shape=torch.Size((100,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862deff-1c8f-4bd0-8713-27f6adea07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 3.5))\n",
    "\n",
    "true_function = ax1.plot(\n",
    "    x_values.numpy(), y_values.numpy(), color=plt.cm.tab10(0),\n",
    "    alpha=0.8, zorder=3, label='true_function'\n",
    ")\n",
    "predictions = ax1.plot(\n",
    "    x_values.numpy(), pred_mean.numpy(), color=plt.cm.tab10(1),\n",
    "    alpha=0.8, zorder=5, label='prior mean'\n",
    ")\n",
    "uncertainty = ax1.fill_between(\n",
    "    x_values.numpy(), pred_lower.numpy(), pred_upper.numpy(),\n",
    "    color=plt.cm.tab10(1), alpha=0.2, zorder=2, label='prior uncertainty'\n",
    ")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x_values.numpy(), pred_samples.T.numpy(), alpha=0.2)\n",
    "ax2.set_xlim((x_values.min(), x_values.max()))\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlim((x_values.min(), x_values.max()))\n",
    "    ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7431c4-d6a9-4000-a4fe-b172069b1996",
   "metadata": {},
   "source": [
    "## Posterior predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9b7c8-6f67-41e6-8a73-c0833d455e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExactInferenceGP(\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    prior_var=0.4,\n",
    "    prior_length=0.1,\n",
    "    noise_var=0.001\n",
    ")\n",
    "\n",
    "print('Prior length: {:.4f}'.format(model.prior_length.item()))\n",
    "print('Prior std.: {:.4f}'.format(torch.sqrt(model.prior_var).item()))\n",
    "print('Noise std.: {:.4f}'.format(torch.sqrt(model.noise_var).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31250dfa-2040-4386-9d97-09dc3c159b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # eval mode for posterior predictions\n",
    "# model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    post_mvn = model(x_values) # compute posterior predictions in eval mode\n",
    "    pred_mvn = model.likelihood(post_mvn) # include likelihood data model\n",
    "\n",
    "    pred_mean = pred_mvn.mean\n",
    "    pred_std = pred_mvn.stddev\n",
    "    pred_var = pred_mvn.variance\n",
    "    pred_cov = pred_mvn.covariance_matrix\n",
    "    pred_lower, pred_upper = pred_mvn.confidence_region() # two stds.\n",
    "\n",
    "    pred_samples = post_mvn.sample(sample_shape=torch.Size((100,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08385b-0438-4d1c-85a3-c1eb82e7898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 3.5))\n",
    "\n",
    "true_function = ax1.plot(\n",
    "    x_values.numpy(), y_values.numpy(),\n",
    "    color=plt.cm.tab10(0), alpha=0.8, zorder=3, label='true_function'\n",
    ")\n",
    "training_data = ax1.scatter(\n",
    "    x_train.numpy(), y_train.numpy(),\n",
    "    color=plt.cm.tab10(0), alpha=0.8, zorder=4, label='training data'\n",
    ")\n",
    "predictions = ax1.plot(\n",
    "    x_values.numpy(), pred_mean.numpy(),\n",
    "    color=plt.cm.tab10(1), alpha=0.8, zorder=5, label='predictions'\n",
    ")\n",
    "uncertainty = ax1.fill_between(\n",
    "    x_values.numpy(), pred_lower.numpy(), pred_upper.numpy(),\n",
    "    color=plt.cm.tab10(1), alpha=0.2, zorder=2, label='uncertainty'\n",
    ")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x_values.numpy(), pred_samples.T.numpy(), alpha=0.2)\n",
    "ax2.set_xlim((x_values.min(), x_values.max()))\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlim((x_values.min(), x_values.max()))\n",
    "    ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d23de-80fc-4c8b-a62f-1a2b3e18ca24",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cb775-fed7-4158-8f8b-ce1176014c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExactInferenceGP(\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    prior_var=0.4,\n",
    "    prior_length=0.1,\n",
    "    noise_var=0.001\n",
    ")\n",
    "\n",
    "print('Prior length: {:.4f}'.format(model.prior_length.item()))\n",
    "print('Prior std.: {:.4f}'.format(torch.sqrt(model.prior_var).item()))\n",
    "print('Noise std.: {:.4f}'.format(torch.sqrt(model.noise_var).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eae996-98e8-438f-b9bf-68efcff0f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9b35f-6579-4bf9-8643-6ec64ad2143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 200\n",
    "\n",
    "model.train() # train mode for (prior) hyperparameter estimation\n",
    "# model.likelihood.train()\n",
    "\n",
    "for idx in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    prior_mvn = model(x_train) # compute (prior) distribution in train mode\n",
    "    loss = -mll(prior_mvn, y_train) # compute negative log-likelihood loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (idx + 1) % 5 == 0 or (idx + 1) in (1, num_iterations):\n",
    "        print('Iteration {:d}, loss: {:.4f}'.format(idx + 1, loss.item()))\n",
    "\n",
    "print('\\nPrior length: {:.4f}'.format(model.prior_length.item()))\n",
    "print('Prior std.: {:.4f}'.format(torch.sqrt(model.prior_var).item()))\n",
    "print('Noise std.: {:.4f}'.format(torch.sqrt(model.noise_var).item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
